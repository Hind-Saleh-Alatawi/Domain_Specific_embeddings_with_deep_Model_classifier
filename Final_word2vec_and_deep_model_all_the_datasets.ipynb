{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hind-Saleh-Alatawi/Domain_Specific_embeddings_with_deep_Model_classifier/blob/main/Final_word2vec_and_deep_model_all_the_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrMuNOY2RAVA",
        "outputId": "5cea2208-c748-455b-e16e-a4551d6193c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHqA0-cP8eKQ",
        "outputId": "5697f227-a8a1-4034-9428-fe4346f3ce03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "pip uninstall tensorflow\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.15.2:\n",
            "  Would remove:\n",
            "    /tensorflow-1.15.2/python3.6/tensorflow-1.15.2.dist-info/*\n",
            "    /tensorflow-1.15.2/python3.6/tensorflow/*\n",
            "    /tensorflow-1.15.2/python3.6/tensorflow_core/*\n",
            "Proceed (y/n)? "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtfBsmAmVhQR"
      },
      "source": [
        "pip install tensorflow==1.14"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKriQdwl_tCo"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z2uh7o6UpJu"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import re\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn import metrics\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from string import punctuation\n",
        "from keras.layers import Dense, Embedding, Flatten,Dropout, Activation,LSTM,Bidirectional,CuDNNLSTM\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.initializers import Constant\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
        "import os\n",
        "import io\n",
        "import numpy as npvocab\n",
        "import gensim\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "xrange=range\n",
        "#NAME ='CNN-word2vec-600Users-50and25Dense'\n",
        "#filepath='Bbalanced_Dataset_20k_20k.csv'\n",
        "#filepath='StormfrontdatasetBALANCED.csv'\n",
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz'\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "'''\n",
        "'Pre-Waseem-EMNLP': '/content/drive/My Drive/hate speech/Datasets Hatespeech/Preprocessed /PreprocessedWaseem-EMNLP.csv',\n",
        "\n",
        "                 'Pre-Waseem-NAACL': '/content/drive/My Drive/hate speech/Datasets Hatespeech/Preprocessed /PreprocessedWaseem-NAACL.csv',\n",
        "\n",
        "                 'Pre-Davidson': '/content/drive/My Drive/hate speech/Datasets Hatespeech/Preprocessed /Preprocessedlabeled_data_Davidson_27000_ready.csv',\n",
        "                 'stormfront': '/content/drive/My Drive/white _supremacist/Stormfrontdataset.csv',\n",
        "                 'ourdataset': '/content/drive/My Drive/white _supremacist/Batch_text_Voting_labels_ready.csv',\n",
        "                 'Pre_Combined_balanced':'/content/drive/My Drive/hate speech/Datasets Hatespeech/Preprocessed /Preprocessed LabeledDavidsonWaseemCombinedBalanced32512.csv',\n",
        "                                  'SubsetCombinedBalancedWS': '/content/drive/My Drive/white _supremacist/BalancedSubsetStormfrontdataset.csv'\n",
        "'Waseem-EMNLP': '/content/drive/My Drive/hate speech/Datasets Hatespeech/Final_Preprocessed_Hate_RemoveUserName/Final_Preprocessed_datasets_WUNWaseem-EMNLP.csv',\n",
        "                 'Waseem-NAACL': '/content/drive/My Drive/hate speech/Datasets Hatespeech/Final_Preprocessed_Hate_RemoveUserName/Final_Preprocessed_datasets_WUNWaseem-NAACL.csv',\n",
        "                 'Combined_balanced':'/content/drive/My Drive/hate speech/Datasets Hatespeech/Final_Preprocessed_Hate_RemoveUserName/Final_Preprocessed_datasets_WUNLabeledDavidsonWaseemCombinedBalanced32512.csv'\n",
        "'''\n",
        "filepath_dict = {\n",
        "                 'PreprocessedWScombined balanced': '/content/drive/My Drive/white _supremacist/Preprocessed/Preprocessed WS datasetscombined balanced ws.csv',\n",
        "                 'PreprocessedStormfrontdataset': '/content/drive/My Drive/white _supremacist/Preprocessed/Preprocessed WS datasetsStormfrontdataset.csv',\n",
        "                 'PreprocessedTwitterdataset': '/content/drive/My Drive/white _supremacist/Preprocessed/Preprocessed WS datasetsBatch_text_Voting_labels_ready.csv'\n",
        "\n",
        "                                 }\n",
        "df_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "    df = pd.read_csv(filepath,names=['text','label'],sep=',',skiprows=[0])#skiprows=[0] to skip header file text,label from each file\n",
        "    df['source'] = source  # Add another column filled with the source name\n",
        "    df_list.append(df)\n",
        "df = pd.concat(df_list)\n",
        "df.to_csv(\"df.csv\")\n",
        "print(df)\n",
        "\n",
        "\n",
        "\n",
        "def clean_doc(doc):\n",
        "    #for row in doc:\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # remove punctuation from each token\n",
        "    table = str.maketrans('', '', punctuation)\n",
        "    tokens = [w.translate(table) for w in tokens]\n",
        "    # filter out tokens not in vocab\n",
        "    #tokens = [w for w in tokens if w in vocab]\n",
        "    tokens = ' '.join(tokens)\n",
        "    return tokens\n",
        "\n",
        "def makedocument(texts):\n",
        "    # load all docs in a directory\n",
        "    documents=list()\n",
        "    for text in texts:\n",
        "     tokens = clean_doc(text)\n",
        "     #print(tokens)\n",
        "     # add to list\n",
        "     documents.append(tokens)\n",
        "    return documents\n",
        "all_outputs=[]\n",
        "from sklearn.model_selection import train_test_split\n",
        "for source in df['source'].unique():\n",
        "    df_source = df[df['source'] == source]\n",
        "    sentences = df_source['text'].values\n",
        "    y = df_source['label'].values\n",
        "    def max_length(lines):\n",
        "        return max([len(s.split()) for s in lines])\n",
        "    DatasetName=source\n",
        "    print(\"Processing dataset....\"+DatasetName)\n",
        "    df_ChoosenDataset = df[df['source'] == DatasetName]\n",
        "    Tweets = df_ChoosenDataset['text'].values\n",
        "    labels = df_ChoosenDataset['label'].values\n",
        "    number_of_labels = df_ChoosenDataset['label'].value_counts()\n",
        "    print('number_of_labels is :\\n',number_of_labels)\n",
        "    #Tweets=makedocument(Tweets)\n",
        "    Tweets=makedocument(Tweets)\n",
        "    Tweets_train, Tweets_test, y_train, y_test = train_test_split(Tweets, labels,test_size=0.2,random_state=1000)# stratify=labels,\n",
        "    print('Tweets_train:',Tweets_train[:5])\n",
        "    #\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\num_words=10000,split=' '\n",
        "    tokenizer = Tokenizer()\n",
        "    allsentences=Tweets_train+Tweets_test\n",
        "    tokenizer.fit_on_texts(allsentences)\n",
        "    for word in ['kill', 'muslim', 'happy', 'black']:\n",
        "        print('{}: {}'.format(word, tokenizer.word_index[word]))\n",
        "    encoded_docs = tokenizer.texts_to_sequences(Tweets_train)\n",
        "    X_test = tokenizer.texts_to_sequences(Tweets_test)\n",
        "    vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
        "    maxlen = max([len(s.split()) for s in Tweets])\n",
        "    word_index = tokenizer.word_index\n",
        "    print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "    X_train = pad_sequences(encoded_docs, padding='post', maxlen=maxlen)\n",
        "    X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "    '''\n",
        "    classifier = LogisticRegression()\n",
        "    classifier.fit(X_train, y_train)\n",
        "    score = classifier.score(X_test, y_test)\n",
        "    print('Accuracy for data: {:.4f}'.format(score))\n",
        "    '''\n",
        "    #____________________________________________________________________\n",
        "    ## I should fix .bin loading\n",
        "    from gensim.models import Word2Vec, KeyedVectors\n",
        "    t_model = KeyedVectors.load_word2vec_format(EMBEDDING_FILE,binary=True)\n",
        "    #EmbeddingFile='/content/drive/My Drive/Models/All_Twitter_Papers_withHateWords_1M_300_CBOW2.txt'\n",
        "    #raw_embedding = load_embedding(EmbeddingFile)\n",
        "    #embedding_vectors = get_weight_matrix(t_model, tokenizer.word_index,embedding_dim)\n",
        "    # create the embedding layer\n",
        "    #print(embedding_vectors)\n",
        "    # define model this is different frm original one\n",
        "    #class_names = [\"HATE\",\"NONHATE\"]\n",
        "    #First, let’s have a quick look how many of the embedding vectors are nonzero:\n",
        "    #nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_vectors, axis=1))\n",
        "    #similarity_percentage=nonzero_elements / vocab_size\n",
        "    #This means 95.1% of the vocabulary is covered by the pretrained model, which is a good coverage of our vocabulary. Let’s have a look at the performance when using the GlobalMaxPool1D layer:\n",
        "    #print(\"{} of the vocabulary is covered by the pretrained model for {}\".format(similarity_percentage,DatasetName))\n",
        "    #to load my model I should use this statement and change each 300 to 100\n",
        "    # t_model = Word2Vec.load('embedding_word2vec_Stormfronttextonly.model')\n",
        "    embeddings_index = {}\n",
        "    for line in Tweets:\n",
        "        for word in line.split(\" \"):\n",
        "            if word in t_model.wv:\n",
        "              coefs = t_model.wv[word]\n",
        "              embeddings_index[word] = coefs\n",
        "            else:embeddings_index[word] = [np.zeros(300)]\n",
        "\n",
        "    EMBEDDING_DIM =300\n",
        "    num_words = len(word_index) + 1\n",
        "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "\n",
        "    for word, i in word_index.items():\n",
        "        if i > num_words:\n",
        "            continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = np.array(embedding_vector)\n",
        "\n",
        "    #\n",
        "    nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
        "    similarity_percentage=nonzero_elements / vocab_size\n",
        "    embedding_layer = Embedding(num_words,EMBEDDING_DIM,weights=[embedding_matrix],input_length=maxlen)#,trainable=True\n",
        "    model = Sequential()\n",
        "    model.add(embedding_layer)\n",
        "    model.add(Bidirectional(CuDNNLSTM(75)))\n",
        "    model.add(Dense(32, activation='linear'))\n",
        "    model.add(Dense(1, activation='sigmoid'))# compile network\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    history=model.fit(X_train, y_train, epochs=10,batch_size=256,validation_data=(X_test, y_test))\n",
        "    print(model.summary())\n",
        "    # compile network\n",
        "    #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])#optimizer=rmsprop\n",
        "    # fit the model\n",
        "    #history=model.fit(X_train, y_train,  epochs=10, verbose=2,validation_data=(X_test, y_test))#\n",
        "    loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(\"Training Accuracy:\",(accuracy),'Training Loss:',loss)\n",
        "    loss,Accuracy=scores\n",
        "    print(\"validation Accuracy:\",Accuracy,' validation loss:',loss)\n",
        "    #plot_history(history)\n",
        "    #plt.show()\n",
        "    # evaluate X_train\n",
        "    loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(\"Testing Accuracy:  {:.4f}\".format(acc))\n",
        "    y_predict=model.predict(X_test)\n",
        "    print(classification_report(y_predict.round(), y_test))\n",
        "    accuracy=accuracy_score(y_test, y_predict.round())\n",
        "    print('DEEP MODEL Accuracy:',accuracy)\n",
        "    precision=precision_score(y_test, y_predict.round())\n",
        "    print('DEEP MODEL precision_score:', precision)\n",
        "    recall=recall_score(y_test, y_predict.round())\n",
        "    print('DEEP MODEL recall_score:',recall )\n",
        "    f1=f1_score(y_test, y_predict.round())\n",
        "    print('DEEP MODEL f1_score:',f1 )\n",
        "    f0=f1_score(y_test, y_predict.round(),pos_label=0)\n",
        "    print('DEEP MODEL f1_score of Zero:',f1 )\n",
        "    fweighted=f1_score(y_test, y_predict.round(),average='weighted')\n",
        "    print('DEEP MODEL f1_score of fweighted:',fweighted )\n",
        "    fMacro=f1_score(y_test, y_predict.round(),average='macro')\n",
        "    print('DEEP MODEL f1_score of fMacro:',fMacro )\n",
        "    fMicro=f1_score(y_test, y_predict.round(),average='micro')\n",
        "    print('DEEP MODEL f1_score of fMicro:',fMicro )\n",
        "    ClassificationReport=classification_report(y_test, y_predict.round())\n",
        "    print('DEEP MODEL ClassificationReport:',fMicro )\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y_test,y_predict.round())\n",
        "    AUC=metrics.auc(fpr, tpr)\n",
        "    print('DEEP MODEL AUC: ' ,AUC)\n",
        "    import seaborn as sns\n",
        "    labels=[\"neutral\",\"hate\"]\n",
        "    cm =confusion_matrix(y_test, y_predict.round())\n",
        "    print(\"confusion matrix\",cm)\n",
        "    index = ['neutral','hate']\n",
        "    columns = ['neutral','hate']\n",
        "    cm_df = pd.DataFrame(cm,columns,index)\n",
        "    plt.figure(figsize=(10,8))\n",
        "    sns.heatmap(cm_df, annot=True,cmap=plt.cm.Blues)\n",
        "    plt.ylabel('True label')\n",
        "    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
        "    misclass = 1 - accuracy\n",
        "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
        "    #plt.show()\n",
        "    csv_file=source+\"ConfusionMatrix\"\n",
        "    plt.savefig(csv_file.split('.csv')[0] + '.png')\n",
        "    from datetime import datetime\n",
        "\n",
        "    now = datetime.now()\n",
        "    model_summary=model.summary()\n",
        "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
        "    def get_model_summary(model):\n",
        "        stream = io.StringIO()\n",
        "        model.summary(print_fn=lambda x: stream.write(x + '\\n'))\n",
        "        summary_string = stream.getvalue()\n",
        "        stream.close()\n",
        "        return summary_string\n",
        "    model_summary_string = get_model_summary(model)\n",
        "    totals = [DatasetName,similarity_percentage, accuracy, precision, recall,f1,f0,fweighted,fMacro,fMicro,AUC,dt_string,model_summary_string,ClassificationReport]\n",
        "    all_outputs.append(totals)\n",
        "    mytest = \"Google hate die\"\n",
        "    encoded_mytest = tokenizer.texts_to_sequences([mytest])\n",
        "    padded_mytest = pad_sequences(encoded_mytest, maxlen=maxlen, padding='post')\n",
        "    yhat = model.predict(padded_mytest)\n",
        "    print('prediction of sentence',yhat)\n",
        "result = pd.DataFrame(data=all_outputs, columns=['Dataset Name','similarity_percentage', 'accuracy', 'precision', 'recall','f1','f0','fweighted','fMacro','fMicro','AUC','today date Time','model summary','ClassificationReport'])\n",
        "result.to_csv(\"FinalPretrainedDeepModelResult\"+datetime.now().strftime(\"%d-%m-%Y_%I-%M-%S_%p\")+\".csv\")\n",
        "result.to_csv(\"/content/drive/My Drive/hate speech/Datasets Hatespeech/\"+\"FinalPretrainedDeepModelResult\"+datetime.now().strftime(\"%d-%m-%Y_%I-%M-%S_%p\")+\".csv\")\n",
        "cm = confusion_matrix(y_test, y_predict.round())\n",
        "\n",
        "print(cm)\n",
        "\n",
        "#Now the normalize the diagonal entries\n",
        "\n",
        "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "print(cm)\n",
        "\n",
        "#The diagonal entries are the accuracies of each class\n",
        "\n",
        "cm.diagonal()\n",
        "\n",
        "#array([1.        , 0.        , 0.66666667])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}